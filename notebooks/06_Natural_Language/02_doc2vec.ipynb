{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gensim.models as g\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhoutao/Documents/PythonDataScienceHandbook/notebooks/06_Natural_Language\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "BASEDIR = os.path.abspath(os.path.curdir)\n",
    "print BASEDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0  # 0 = dbow; 1 = dmpv\n",
    "worker_count = 1  # number of parallel processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pretrained word embeddings\n",
    "pretrained_emb = BASEDIR + \"/doc2vecdata/pretrained_word_embeddings.txt\"\n",
    "\n",
    "# None if use without pretrained embeddings\n",
    "\n",
    "# input corpus\n",
    "train_corpus = BASEDIR + \"/doc2vecdata/train_docs.txt\"\n",
    "\n",
    "# output model\n",
    "saved_path =BASEDIR + \"/doc2vecdata/model.bin\"\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-05 16:02:36,976 : INFO : collecting all words and their counts\n",
      "2017-08-05 16:02:37,011 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2017-08-05 16:02:37,070 : INFO : collected 11097 word types and 1000 unique tags from a corpus of 1000 examples and 84408 words\n",
      "2017-08-05 16:02:37,071 : INFO : Loading a fresh vocabulary\n",
      "2017-08-05 16:02:37,089 : INFO : min_count=1 retains 11097 unique words (100% of original 11097, drops 0)\n",
      "2017-08-05 16:02:37,089 : INFO : min_count=1 leaves 84408 word corpus (100% of original 84408, drops 0)\n",
      "2017-08-05 16:02:37,116 : INFO : deleting the raw counts dictionary of 11097 items\n",
      "2017-08-05 16:02:37,117 : INFO : sample=1e-05 downsamples 3599 most-common words\n",
      "2017-08-05 16:02:37,118 : INFO : downsampling leaves estimated 22704 word corpus (26.9% of prior 84408)\n",
      "2017-08-05 16:02:37,118 : INFO : estimated required memory for 11097 words and 300 dimensions: 33381300 bytes\n",
      "2017-08-05 16:02:37,142 : INFO : resetting layer weights\n",
      "2017-08-05 16:02:37,273 : INFO : training model with 1 workers on 11098 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2017-08-05 16:02:38,274 : INFO : PROGRESS: at 2.10% examples, 50204 words/s, in_qsize 2, out_qsize 0\n",
      "2017-08-05 16:02:39,322 : INFO : PROGRESS: at 4.10% examples, 47607 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:40,365 : INFO : PROGRESS: at 6.36% examples, 48757 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:41,383 : INFO : PROGRESS: at 8.48% examples, 48853 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:42,420 : INFO : PROGRESS: at 10.71% examples, 49238 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:43,425 : INFO : PROGRESS: at 12.83% examples, 49342 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:44,478 : INFO : PROGRESS: at 14.96% examples, 49109 words/s, in_qsize 2, out_qsize 0\n",
      "2017-08-05 16:02:45,531 : INFO : PROGRESS: at 17.04% examples, 48895 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:46,542 : INFO : PROGRESS: at 19.14% examples, 48970 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:47,596 : INFO : PROGRESS: at 21.43% examples, 49117 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:48,632 : INFO : PROGRESS: at 23.64% examples, 49291 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:49,680 : INFO : PROGRESS: at 25.89% examples, 49397 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:50,684 : INFO : PROGRESS: at 28.00% examples, 49441 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:51,686 : INFO : PROGRESS: at 30.00% examples, 49277 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:52,695 : INFO : PROGRESS: at 32.10% examples, 49317 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:53,735 : INFO : PROGRESS: at 34.36% examples, 49435 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:54,780 : INFO : PROGRESS: at 36.48% examples, 49360 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:55,782 : INFO : PROGRESS: at 38.48% examples, 49246 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:56,824 : INFO : PROGRESS: at 40.72% examples, 49307 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:57,876 : INFO : PROGRESS: at 42.83% examples, 49227 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:58,925 : INFO : PROGRESS: at 45.04% examples, 49296 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:02:59,979 : INFO : PROGRESS: at 47.29% examples, 49347 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:01,021 : INFO : PROGRESS: at 49.54% examples, 49420 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:02,059 : INFO : PROGRESS: at 51.76% examples, 49469 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:03,110 : INFO : PROGRESS: at 54.00% examples, 49514 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:04,171 : INFO : PROGRESS: at 56.10% examples, 49428 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:05,192 : INFO : PROGRESS: at 58.23% examples, 49425 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:06,235 : INFO : PROGRESS: at 60.48% examples, 49483 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:07,240 : INFO : PROGRESS: at 62.59% examples, 49499 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:08,269 : INFO : PROGRESS: at 64.83% examples, 49551 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:09,298 : INFO : PROGRESS: at 67.04% examples, 49614 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:10,320 : INFO : PROGRESS: at 69.29% examples, 49690 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:11,367 : INFO : PROGRESS: at 71.54% examples, 49724 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:12,405 : INFO : PROGRESS: at 73.76% examples, 49756 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:13,444 : INFO : PROGRESS: at 76.00% examples, 49796 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:14,485 : INFO : PROGRESS: at 78.10% examples, 49749 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:15,514 : INFO : PROGRESS: at 80.23% examples, 49729 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:16,555 : INFO : PROGRESS: at 82.48% examples, 49765 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:17,563 : INFO : PROGRESS: at 84.59% examples, 49767 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:18,585 : INFO : PROGRESS: at 86.83% examples, 49812 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:19,610 : INFO : PROGRESS: at 89.04% examples, 49861 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:20,651 : INFO : PROGRESS: at 91.29% examples, 49890 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:21,678 : INFO : PROGRESS: at 93.54% examples, 49935 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:22,693 : INFO : PROGRESS: at 95.76% examples, 49977 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:23,703 : INFO : PROGRESS: at 98.00% examples, 50036 words/s, in_qsize 1, out_qsize 0\n",
      "2017-08-05 16:03:24,605 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-08-05 16:03:24,605 : INFO : training on 8440800 raw words (2370696 effective words) took 47.3s, 50087 effective words/s\n",
      "2017-08-05 16:03:24,606 : INFO : saving Doc2Vec object under /home/zhoutao/Documents/PythonDataScienceHandbook/notebooks/06_Natural_Language/doc2vecdata/model.bin, separately None\n",
      "2017-08-05 16:03:24,606 : INFO : not storing attribute syn0norm\n",
      "2017-08-05 16:03:24,609 : INFO : not storing attribute cum_table\n",
      "2017-08-05 16:03:24,687 : INFO : saved /home/zhoutao/Documents/PythonDataScienceHandbook/notebooks/06_Natural_Language/doc2vecdata/model.bin\n"
     ]
    }
   ],
   "source": [
    "# train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold,\n",
    "                  workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1,\n",
    "                  iter=train_epoch)\n",
    "\n",
    "# save model\n",
    "model.save(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载参数 parameters\n",
    "model=BASEDIR + \"/doc2vecdata/model.bin\"\n",
    "test_docs=BASEDIR + \"/doc2vecdata/test_docs.txt\"\n",
    "output_file=BASEDIR + \"/doc2vecdata/test_vectors.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "print m.wv.most_similar(positive=['family', 'dog'])\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
