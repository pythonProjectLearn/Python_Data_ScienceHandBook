{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建文本特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo')\n",
      "('is', 'a', 'foo', 'bar')\n",
      "('a', 'foo', 'bar', 'sentences')\n",
      "('foo', 'bar', 'sentences', 'and')\n",
      "('bar', 'sentences', 'and', 'i')\n",
      "('sentences', 'and', 'i', 'want')\n",
      "('and', 'i', 'want', 'to')\n",
      "('i', 'want', 'to', 'ngramize')\n",
      "('want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 4 # you can give 4, 5, 1 or any number less than sentences length\n",
    "ngramsres = ngrams(sentence.split(), n)\n",
    "for grams in ngramsres:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 bagofwords词袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 1 1 0]\n",
      " [1 1 0 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# analyzer是构建词袋模型，所使的训练集文本\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2), min_df=1)\n",
    "\n",
    "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
    "# this check weather the given word character is present in the above teo word which are documents here.\n",
    "ngram_vectorizer.get_feature_names() == ([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'])\n",
    "print(counts.toarray().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 one-hot编码\n",
    "\n",
    "X[['country']]返回的是dataframe;  \n",
    "\n",
    "X['country']返回的是seriese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country=US': 2, 'country=CAN': 0, 'country=MEX': 1}\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "X = pd.DataFrame({'income': [100000,110000,90000,30000,14000,50000],\n",
    "                  'country':['US', 'CAN', 'US', 'CAN', 'MEX', 'US'],\n",
    "                  'race':['White', 'Black', 'Latino', 'White', 'White', 'Black']})\n",
    "v = DictVectorizer()\n",
    "X_qual = v.fit_transform(X[['country']].to_dict('records'))\n",
    "print(v.vocabulary_)\n",
    "print( X_qual.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 doc2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import gensim.models as g\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 模型参数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec parameters\n",
    "vector_size = 300\n",
    "window_size = 15\n",
    "min_count = 1\n",
    "sampling_threshold = 1e-5\n",
    "negative_size = 5\n",
    "train_epoch = 100\n",
    "dm = 0  # 0 = dbow; 1 = dmpv\n",
    "worker_count = 1  # number of parallel processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该语料中，词汇用空格保存\n",
    "train_corpus = \"/home/zt/Documents/Data/NLTK/doc2vecdata/train_docs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zt/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:362: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "/home/zt/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "2018-03-18 20:03:51,847 : INFO : collecting all words and their counts\n",
      "2018-03-18 20:03:51,851 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-03-18 20:03:51,878 : INFO : collected 11097 word types and 1000 unique tags from a corpus of 1000 examples and 84408 words\n",
      "2018-03-18 20:03:51,879 : INFO : Loading a fresh vocabulary\n",
      "2018-03-18 20:03:51,897 : INFO : min_count=1 retains 11097 unique words (100% of original 11097, drops 0)\n",
      "2018-03-18 20:03:51,898 : INFO : min_count=1 leaves 84408 word corpus (100% of original 84408, drops 0)\n",
      "2018-03-18 20:03:51,923 : INFO : deleting the raw counts dictionary of 11097 items\n",
      "2018-03-18 20:03:51,925 : INFO : sample=1e-05 downsamples 3599 most-common words\n",
      "2018-03-18 20:03:51,926 : INFO : downsampling leaves estimated 22704 word corpus (26.9% of prior 84408)\n",
      "2018-03-18 20:03:51,943 : INFO : estimated required memory for 11097 words and 300 dimensions: 33381300 bytes\n",
      "2018-03-18 20:03:51,944 : INFO : resetting layer weights\n",
      "2018-03-18 20:03:52,054 : INFO : training model with 1 workers on 11098 vocabulary and 300 features, using sg=1 hs=0 sample=1e-05 negative=5 window=15\n",
      "2018-03-18 20:03:52,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:52,548 : INFO : EPOCH - 1 : training on 84408 raw words (23740 effective words) took 0.5s, 48222 effective words/s\n",
      "2018-03-18 20:03:53,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:53,033 : INFO : EPOCH - 2 : training on 84408 raw words (23650 effective words) took 0.5s, 48887 effective words/s\n",
      "2018-03-18 20:03:53,548 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:53,548 : INFO : EPOCH - 3 : training on 84408 raw words (23619 effective words) took 0.5s, 45916 effective words/s\n",
      "2018-03-18 20:03:54,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:54,037 : INFO : EPOCH - 4 : training on 84408 raw words (23652 effective words) took 0.5s, 48474 effective words/s\n",
      "2018-03-18 20:03:54,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:54,510 : INFO : EPOCH - 5 : training on 84408 raw words (23580 effective words) took 0.5s, 49958 effective words/s\n",
      "2018-03-18 20:03:54,989 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:54,989 : INFO : EPOCH - 6 : training on 84408 raw words (23768 effective words) took 0.5s, 49696 effective words/s\n",
      "2018-03-18 20:03:55,457 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:55,457 : INFO : EPOCH - 7 : training on 84408 raw words (23697 effective words) took 0.5s, 50865 effective words/s\n",
      "2018-03-18 20:03:55,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:55,934 : INFO : EPOCH - 8 : training on 84408 raw words (23643 effective words) took 0.5s, 49766 effective words/s\n",
      "2018-03-18 20:03:56,402 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:56,402 : INFO : EPOCH - 9 : training on 84408 raw words (23579 effective words) took 0.5s, 50518 effective words/s\n",
      "2018-03-18 20:03:56,875 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:56,875 : INFO : EPOCH - 10 : training on 84408 raw words (23674 effective words) took 0.5s, 50099 effective words/s\n",
      "2018-03-18 20:03:57,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:57,351 : INFO : EPOCH - 11 : training on 84408 raw words (23720 effective words) took 0.5s, 50031 effective words/s\n",
      "2018-03-18 20:03:57,827 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:57,828 : INFO : EPOCH - 12 : training on 84408 raw words (23711 effective words) took 0.5s, 49841 effective words/s\n",
      "2018-03-18 20:03:58,304 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:58,305 : INFO : EPOCH - 13 : training on 84408 raw words (23598 effective words) took 0.5s, 49540 effective words/s\n",
      "2018-03-18 20:03:58,774 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:58,774 : INFO : EPOCH - 14 : training on 84408 raw words (23691 effective words) took 0.5s, 50579 effective words/s\n",
      "2018-03-18 20:03:59,263 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:59,263 : INFO : EPOCH - 15 : training on 84408 raw words (23755 effective words) took 0.5s, 48636 effective words/s\n",
      "2018-03-18 20:03:59,752 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:03:59,752 : INFO : EPOCH - 16 : training on 84408 raw words (23605 effective words) took 0.5s, 48385 effective words/s\n",
      "2018-03-18 20:04:00,231 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:00,232 : INFO : EPOCH - 17 : training on 84408 raw words (23645 effective words) took 0.5s, 49363 effective words/s\n",
      "2018-03-18 20:04:00,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:00,711 : INFO : EPOCH - 18 : training on 84408 raw words (23656 effective words) took 0.5s, 49536 effective words/s\n",
      "2018-03-18 20:04:01,186 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:01,186 : INFO : EPOCH - 19 : training on 84408 raw words (23646 effective words) took 0.5s, 49933 effective words/s\n",
      "2018-03-18 20:04:01,669 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:01,669 : INFO : EPOCH - 20 : training on 84408 raw words (23645 effective words) took 0.5s, 49016 effective words/s\n",
      "2018-03-18 20:04:02,143 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:02,143 : INFO : EPOCH - 21 : training on 84408 raw words (23647 effective words) took 0.5s, 49978 effective words/s\n",
      "2018-03-18 20:04:02,621 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:02,622 : INFO : EPOCH - 22 : training on 84408 raw words (23759 effective words) took 0.5s, 49856 effective words/s\n",
      "2018-03-18 20:04:03,095 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:03,095 : INFO : EPOCH - 23 : training on 84408 raw words (23638 effective words) took 0.5s, 50007 effective words/s\n",
      "2018-03-18 20:04:03,567 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:03,567 : INFO : EPOCH - 24 : training on 84408 raw words (23759 effective words) took 0.5s, 50377 effective words/s\n",
      "2018-03-18 20:04:04,046 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:04,046 : INFO : EPOCH - 25 : training on 84408 raw words (23859 effective words) took 0.5s, 49912 effective words/s\n",
      "2018-03-18 20:04:04,521 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:04,522 : INFO : EPOCH - 26 : training on 84408 raw words (23691 effective words) took 0.5s, 50010 effective words/s\n",
      "2018-03-18 20:04:04,998 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:04,999 : INFO : EPOCH - 27 : training on 84408 raw words (23647 effective words) took 0.5s, 49658 effective words/s\n",
      "2018-03-18 20:04:05,470 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:05,471 : INFO : EPOCH - 28 : training on 84408 raw words (23749 effective words) took 0.5s, 50462 effective words/s\n",
      "2018-03-18 20:04:05,942 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:05,942 : INFO : EPOCH - 29 : training on 84408 raw words (23552 effective words) took 0.5s, 50070 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 20:04:06,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:06,422 : INFO : EPOCH - 30 : training on 84408 raw words (23625 effective words) took 0.5s, 49296 effective words/s\n",
      "2018-03-18 20:04:06,899 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:06,899 : INFO : EPOCH - 31 : training on 84408 raw words (23690 effective words) took 0.5s, 49727 effective words/s\n",
      "2018-03-18 20:04:07,381 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:07,381 : INFO : EPOCH - 32 : training on 84408 raw words (23772 effective words) took 0.5s, 49390 effective words/s\n",
      "2018-03-18 20:04:07,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:07,853 : INFO : EPOCH - 33 : training on 84408 raw words (23742 effective words) took 0.5s, 50390 effective words/s\n",
      "2018-03-18 20:04:08,319 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:08,320 : INFO : EPOCH - 34 : training on 84408 raw words (23695 effective words) took 0.5s, 51022 effective words/s\n",
      "2018-03-18 20:04:08,802 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:08,802 : INFO : EPOCH - 35 : training on 84408 raw words (23736 effective words) took 0.5s, 49264 effective words/s\n",
      "2018-03-18 20:04:09,281 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:09,281 : INFO : EPOCH - 36 : training on 84408 raw words (23741 effective words) took 0.5s, 49732 effective words/s\n",
      "2018-03-18 20:04:09,752 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:09,752 : INFO : EPOCH - 37 : training on 84408 raw words (23740 effective words) took 0.5s, 50738 effective words/s\n",
      "2018-03-18 20:04:10,222 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:10,222 : INFO : EPOCH - 38 : training on 84408 raw words (23639 effective words) took 0.5s, 50369 effective words/s\n",
      "2018-03-18 20:04:10,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:10,696 : INFO : EPOCH - 39 : training on 84408 raw words (23703 effective words) took 0.5s, 50223 effective words/s\n",
      "2018-03-18 20:04:11,162 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:11,162 : INFO : EPOCH - 40 : training on 84408 raw words (23608 effective words) took 0.5s, 50759 effective words/s\n",
      "2018-03-18 20:04:11,638 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:11,639 : INFO : EPOCH - 41 : training on 84408 raw words (23651 effective words) took 0.5s, 49793 effective words/s\n",
      "2018-03-18 20:04:12,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:12,108 : INFO : EPOCH - 42 : training on 84408 raw words (23713 effective words) took 0.5s, 50637 effective words/s\n",
      "2018-03-18 20:04:12,584 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:12,584 : INFO : EPOCH - 43 : training on 84408 raw words (23803 effective words) took 0.5s, 50038 effective words/s\n",
      "2018-03-18 20:04:13,063 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:13,063 : INFO : EPOCH - 44 : training on 84408 raw words (23624 effective words) took 0.5s, 49715 effective words/s\n",
      "2018-03-18 20:04:13,536 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:13,537 : INFO : EPOCH - 45 : training on 84408 raw words (23853 effective words) took 0.5s, 50611 effective words/s\n",
      "2018-03-18 20:04:14,006 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:14,007 : INFO : EPOCH - 46 : training on 84408 raw words (23692 effective words) took 0.5s, 50538 effective words/s\n",
      "2018-03-18 20:04:14,469 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:14,470 : INFO : EPOCH - 47 : training on 84408 raw words (23694 effective words) took 0.5s, 51258 effective words/s\n",
      "2018-03-18 20:04:14,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:14,941 : INFO : EPOCH - 48 : training on 84408 raw words (23883 effective words) took 0.5s, 50917 effective words/s\n",
      "2018-03-18 20:04:15,402 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:15,403 : INFO : EPOCH - 49 : training on 84408 raw words (23530 effective words) took 0.5s, 51000 effective words/s\n",
      "2018-03-18 20:04:15,863 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:15,864 : INFO : EPOCH - 50 : training on 84408 raw words (23694 effective words) took 0.5s, 51435 effective words/s\n",
      "2018-03-18 20:04:16,335 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:16,335 : INFO : EPOCH - 51 : training on 84408 raw words (23651 effective words) took 0.5s, 50253 effective words/s\n",
      "2018-03-18 20:04:16,797 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:16,798 : INFO : EPOCH - 52 : training on 84408 raw words (23727 effective words) took 0.5s, 51397 effective words/s\n",
      "2018-03-18 20:04:17,258 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:17,258 : INFO : EPOCH - 53 : training on 84408 raw words (23705 effective words) took 0.5s, 51528 effective words/s\n",
      "2018-03-18 20:04:17,715 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:17,715 : INFO : EPOCH - 54 : training on 84408 raw words (23723 effective words) took 0.5s, 51970 effective words/s\n",
      "2018-03-18 20:04:18,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:18,192 : INFO : EPOCH - 55 : training on 84408 raw words (23740 effective words) took 0.5s, 49895 effective words/s\n",
      "2018-03-18 20:04:18,653 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:18,654 : INFO : EPOCH - 56 : training on 84408 raw words (23695 effective words) took 0.5s, 51394 effective words/s\n",
      "2018-03-18 20:04:19,112 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:19,112 : INFO : EPOCH - 57 : training on 84408 raw words (23704 effective words) took 0.5s, 51772 effective words/s\n",
      "2018-03-18 20:04:19,583 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:19,584 : INFO : EPOCH - 58 : training on 84408 raw words (23747 effective words) took 0.5s, 50467 effective words/s\n",
      "2018-03-18 20:04:20,049 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:20,049 : INFO : EPOCH - 59 : training on 84408 raw words (23693 effective words) took 0.5s, 50988 effective words/s\n",
      "2018-03-18 20:04:20,509 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:20,509 : INFO : EPOCH - 60 : training on 84408 raw words (23766 effective words) took 0.5s, 51737 effective words/s\n",
      "2018-03-18 20:04:20,975 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:20,975 : INFO : EPOCH - 61 : training on 84408 raw words (23851 effective words) took 0.5s, 51246 effective words/s\n",
      "2018-03-18 20:04:21,438 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:21,439 : INFO : EPOCH - 62 : training on 84408 raw words (23707 effective words) took 0.5s, 51225 effective words/s\n",
      "2018-03-18 20:04:21,902 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:21,903 : INFO : EPOCH - 63 : training on 84408 raw words (23613 effective words) took 0.5s, 51000 effective words/s\n",
      "2018-03-18 20:04:22,356 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:22,357 : INFO : EPOCH - 64 : training on 84408 raw words (23530 effective words) took 0.5s, 51856 effective words/s\n",
      "2018-03-18 20:04:22,818 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:22,818 : INFO : EPOCH - 65 : training on 84408 raw words (23816 effective words) took 0.5s, 51702 effective words/s\n",
      "2018-03-18 20:04:23,277 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:23,278 : INFO : EPOCH - 66 : training on 84408 raw words (23767 effective words) took 0.5s, 51974 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 20:04:23,732 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:23,733 : INFO : EPOCH - 67 : training on 84408 raw words (23622 effective words) took 0.5s, 52006 effective words/s\n",
      "2018-03-18 20:04:24,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:24,189 : INFO : EPOCH - 68 : training on 84408 raw words (23583 effective words) took 0.5s, 51775 effective words/s\n",
      "2018-03-18 20:04:24,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:24,671 : INFO : EPOCH - 69 : training on 84408 raw words (23776 effective words) took 0.5s, 49487 effective words/s\n",
      "2018-03-18 20:04:25,136 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:25,137 : INFO : EPOCH - 70 : training on 84408 raw words (23846 effective words) took 0.5s, 51337 effective words/s\n",
      "2018-03-18 20:04:25,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:25,611 : INFO : EPOCH - 71 : training on 84408 raw words (23750 effective words) took 0.5s, 50148 effective words/s\n",
      "2018-03-18 20:04:26,073 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:26,073 : INFO : EPOCH - 72 : training on 84408 raw words (23696 effective words) took 0.5s, 51344 effective words/s\n",
      "2018-03-18 20:04:26,531 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:26,532 : INFO : EPOCH - 73 : training on 84408 raw words (23873 effective words) took 0.5s, 52148 effective words/s\n",
      "2018-03-18 20:04:26,987 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:26,987 : INFO : EPOCH - 74 : training on 84408 raw words (23756 effective words) took 0.5s, 52265 effective words/s\n",
      "2018-03-18 20:04:27,441 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:27,442 : INFO : EPOCH - 75 : training on 84408 raw words (23639 effective words) took 0.5s, 52085 effective words/s\n",
      "2018-03-18 20:04:27,914 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:27,914 : INFO : EPOCH - 76 : training on 84408 raw words (23739 effective words) took 0.5s, 50300 effective words/s\n",
      "2018-03-18 20:04:28,381 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:28,382 : INFO : EPOCH - 77 : training on 84408 raw words (23756 effective words) took 0.5s, 50961 effective words/s\n",
      "2018-03-18 20:04:28,834 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:28,834 : INFO : EPOCH - 78 : training on 84408 raw words (23555 effective words) took 0.4s, 52729 effective words/s\n",
      "2018-03-18 20:04:29,293 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:29,293 : INFO : EPOCH - 79 : training on 84408 raw words (23690 effective words) took 0.5s, 51693 effective words/s\n",
      "2018-03-18 20:04:29,748 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:29,748 : INFO : EPOCH - 80 : training on 84408 raw words (23673 effective words) took 0.5s, 52152 effective words/s\n",
      "2018-03-18 20:04:30,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:30,201 : INFO : EPOCH - 81 : training on 84408 raw words (23681 effective words) took 0.5s, 52438 effective words/s\n",
      "2018-03-18 20:04:30,669 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:30,669 : INFO : EPOCH - 82 : training on 84408 raw words (23832 effective words) took 0.5s, 51109 effective words/s\n",
      "2018-03-18 20:04:31,129 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:31,130 : INFO : EPOCH - 83 : training on 84408 raw words (23887 effective words) took 0.5s, 52573 effective words/s\n",
      "2018-03-18 20:04:31,591 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:31,592 : INFO : EPOCH - 84 : training on 84408 raw words (23716 effective words) took 0.5s, 51472 effective words/s\n",
      "2018-03-18 20:04:32,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:32,053 : INFO : EPOCH - 85 : training on 84408 raw words (23784 effective words) took 0.5s, 51689 effective words/s\n",
      "2018-03-18 20:04:32,514 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:32,515 : INFO : EPOCH - 86 : training on 84408 raw words (23725 effective words) took 0.5s, 51598 effective words/s\n",
      "2018-03-18 20:04:32,965 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:32,966 : INFO : EPOCH - 87 : training on 84408 raw words (23652 effective words) took 0.5s, 52540 effective words/s\n",
      "2018-03-18 20:04:33,426 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:33,427 : INFO : EPOCH - 88 : training on 84408 raw words (23855 effective words) took 0.5s, 51828 effective words/s\n",
      "2018-03-18 20:04:33,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:33,882 : INFO : EPOCH - 89 : training on 84408 raw words (23598 effective words) took 0.5s, 51966 effective words/s\n",
      "2018-03-18 20:04:34,330 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:34,331 : INFO : EPOCH - 90 : training on 84408 raw words (23600 effective words) took 0.4s, 52628 effective words/s\n",
      "2018-03-18 20:04:34,783 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:34,784 : INFO : EPOCH - 91 : training on 84408 raw words (23829 effective words) took 0.5s, 52658 effective words/s\n",
      "2018-03-18 20:04:35,230 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:35,231 : INFO : EPOCH - 92 : training on 84408 raw words (23689 effective words) took 0.4s, 53101 effective words/s\n",
      "2018-03-18 20:04:35,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:35,683 : INFO : EPOCH - 93 : training on 84408 raw words (23954 effective words) took 0.5s, 53009 effective words/s\n",
      "2018-03-18 20:04:36,141 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:36,141 : INFO : EPOCH - 94 : training on 84408 raw words (23740 effective words) took 0.5s, 51917 effective words/s\n",
      "2018-03-18 20:04:36,599 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:36,599 : INFO : EPOCH - 95 : training on 84408 raw words (23639 effective words) took 0.5s, 51701 effective words/s\n",
      "2018-03-18 20:04:37,050 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:37,050 : INFO : EPOCH - 96 : training on 84408 raw words (23672 effective words) took 0.5s, 52599 effective words/s\n",
      "2018-03-18 20:04:37,518 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:37,518 : INFO : EPOCH - 97 : training on 84408 raw words (23762 effective words) took 0.5s, 50829 effective words/s\n",
      "2018-03-18 20:04:37,978 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:37,979 : INFO : EPOCH - 98 : training on 84408 raw words (23675 effective words) took 0.5s, 51508 effective words/s\n",
      "2018-03-18 20:04:38,432 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:38,433 : INFO : EPOCH - 99 : training on 84408 raw words (23771 effective words) took 0.4s, 53011 effective words/s\n",
      "2018-03-18 20:04:38,885 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-18 20:04:38,885 : INFO : EPOCH - 100 : training on 84408 raw words (23843 effective words) took 0.5s, 52734 effective words/s\n",
      "2018-03-18 20:04:38,886 : INFO : training on a 8440800 raw words (2370696 effective words) took 46.8s, 50623 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train doc2vec model\n",
    "docs = g.doc2vec.TaggedLineDocument(train_corpus)\n",
    "model = g.Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold,\n",
    "                  workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1,\n",
    "                  iter=train_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.3 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 20:07:26,626 : INFO : saving Doc2Vec object under doc2vec_model.bin, separately None\n",
      "2018-03-18 20:07:26,827 : INFO : saved doc2vec_model.bin\n"
     ]
    }
   ],
   "source": [
    "saved_path = \"doc2vec_model.bin\"\n",
    "model.save(saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.4 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 20:12:18,761 : INFO : loading Doc2Vec object from doc2vec_model.bin\n",
      "2018-03-18 20:12:18,938 : INFO : loading vocabulary recursively from doc2vec_model.bin.vocabulary.* with mmap=None\n",
      "2018-03-18 20:12:18,939 : INFO : loading trainables recursively from doc2vec_model.bin.trainables.* with mmap=None\n",
      "2018-03-18 20:12:18,939 : INFO : loading wv recursively from doc2vec_model.bin.wv.* with mmap=None\n",
      "2018-03-18 20:12:18,939 : INFO : loading docvecs recursively from doc2vec_model.bin.docvecs.* with mmap=None\n",
      "2018-03-18 20:12:18,940 : INFO : loaded doc2vec_model.bin\n",
      "2018-03-18 20:12:18,951 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('peach', 0.7730699777603149), ('clingstone', 0.772639811038971), ('harp', 0.7716166973114014), ('bag', 0.7704375982284546), ('harmonica', 0.7704144716262817), ('tow', 0.7691656351089478), ('burlap', 0.7691220045089722), ('plum', 0.7690191268920898), ('dragonfly', 0.768700897693634), ('irons', 0.7677907347679138)]\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "model=\"doc2vec_model.bin\"\n",
    "test_docs=\"/home/zt/Documents/Data/NLTK/doc2vecdata/test_docs.txt\"\n",
    "output_file=\"test_vectors.txt\"\n",
    "\n",
    "#inference hyper-parameters\n",
    "start_alpha=0.01\n",
    "infer_epoch=1000\n",
    "\n",
    "#load model\n",
    "m = g.Doc2Vec.load(model)\n",
    "print(m.wv.most_similar(positive=['family', 'dog']))\n",
    "test_docs = [ x.strip().split() for x in codecs.open(test_docs, \"r\", \"utf-8\").readlines() ]\n",
    "\n",
    "#infer test vectors\n",
    "output = open(output_file, \"w\")\n",
    "for d in test_docs:\n",
    "    output.write( \" \".join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + \"\\n\" )\n",
    "output.flush()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
