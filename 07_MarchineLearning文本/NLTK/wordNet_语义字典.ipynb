{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 词性转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordtokenization():\n",
    "    content = \"\"\"Stemming is funnier than a bummer says the sushi loving computer scientist.\n",
    "    She really wants to buy cars. She told me angrily. It is better for you.\n",
    "    Man is walking. We are meeting tomorrow. You really don't know..!\"\"\"\n",
    "    print(word_tokenize(content))  # 分词\n",
    "\n",
    "def wordlemmatization():\n",
    "    \"\"\"WordNetLemmatizer()改变词性\"\"\"\n",
    "    wordlemma = WordNetLemmatizer()\n",
    "    print(wordlemma.lemmatize('cars'))\n",
    "    print(wordlemma.lemmatize('walking',pos='v'))  # 把walking转化成为v动词\n",
    "    print(wordlemma.lemmatize('meeting',pos='n'))\n",
    "    print(wordlemma.lemmatize('meeting',pos='v'))\n",
    "    print(wordlemma.lemmatize('better',pos='a'))\n",
    "    print(wordlemma.lemmatize('is',pos='v'))\n",
    "    print(wordlemma.lemmatize('funnier',pos='a'))\n",
    "    print(wordlemma.lemmatize('expected',pos='v'))\n",
    "    print(wordlemma.lemmatize('fantasized',pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemming', 'is', 'funnier', 'than', 'a', 'bummer', 'says', 'the', 'sushi', 'loving', 'computer', 'scientist', '.', 'She', 'really', 'wants', 'to', 'buy', 'cars', '.', 'She', 'told', 'me', 'angrily', '.', 'It', 'is', 'better', 'for', 'you', '.', 'Man', 'is', 'walking', '.', 'We', 'are', 'meeting', 'tomorrow', '.', 'You', 'really', 'do', \"n't\", 'know..', '!']\n",
      "\n",
      "\n",
      "----------Word Lemmatization----------\n",
      "car\n",
      "walk\n",
      "meeting\n",
      "meet\n",
      "good\n",
      "be\n",
      "funny\n",
      "expect\n",
      "fantasize\n"
     ]
    }
   ],
   "source": [
    "wordtokenization()\n",
    "print(\"\\n\")\n",
    "print(\"----------Word Lemmatization----------\")\n",
    "wordlemmatization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 句法结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "from nltk.tree import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def definegrammar_pasrereult():\n",
    "    Grammar = nltk.CFG.fromstring(\"\"\" \n",
    "    S -> NP VP \n",
    "    PP -> P NP \n",
    "    NP -> Det N | Det N PP | 'I' \n",
    "    VP -> V NP | VP PP \n",
    "    Det -> 'an' | 'my' \n",
    "    N -> 'elephant' | 'pajamas' \n",
    "    V -> 'shot' \n",
    "    P -> 'in' \n",
    "    \"\"\")\n",
    "    sent = \"I shot an elephant\".split()\n",
    "    parser = nltk.ChartParser(Grammar)\n",
    "    trees = parser.parse(sent)\n",
    "    for tree in trees:\n",
    "        print(tree)\n",
    "\n",
    "# Part 2: Draw the parse tree\n",
    "def draw_parser_tree():\n",
    "    dp1 = Tree('dp', [Tree('d', ['the']), Tree('np', ['dog'])])\n",
    "    dp2 = Tree('dp', [Tree('d', ['the']), Tree('np', ['cat'])])\n",
    "    vp = Tree('vp', [Tree('v', ['chased']), dp2])\n",
    "    tree = Tree('s', [dp1, vp])\n",
    "    print(tree)\n",
    "    print(tree.pformat_latex_qtree())\n",
    "    tree.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Parsing result as per defined grammar-------\n",
      "(S (NP I) (VP (V shot) (NP (Det an) (N elephant))))\n",
      "\n",
      "--------Drawing Parse Tree-------\n",
      "(s (dp (d the) (np dog)) (vp (v chased) (dp (d the) (np cat))))\n",
      "\\Tree [.s\n",
      "        [.dp [.d the ] [.np dog ] ]\n",
      "        [.vp [.v chased ] [.dp [.d the ] [.np cat ] ] ] ]\n",
      "              s               \n",
      "      ________|_____           \n",
      "     |              vp        \n",
      "     |         _____|___       \n",
      "     dp       |         dp    \n",
      "  ___|___     |      ___|___   \n",
      " d       np   v     d       np\n",
      " |       |    |     |       |  \n",
      "the     dog chased the     cat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--------Parsing result as per defined grammar-------\")\n",
    "definegrammar_pasrereult()\n",
    "print(\"\\n--------Drawing Parse Tree-------\")\n",
    "draw_parser_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import pprint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.1 取得语料treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "print( tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 针对句子，构建词特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    \" sentence: [w1, w2, ...], index: the index of the word \"\n",
    "    return {\n",
    "    'word': sentence[index], # 当前词\n",
    "    'is_first': index == 0,\n",
    "    'is_last': index == len(sentence) - 1,\n",
    "    'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "    'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "    'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "    'prefix-1': sentence[index][0],\n",
    "    'prefix-2': sentence[index][:2],\n",
    "    'prefix-3': sentence[index][:3],\n",
    "    'suffix-1': sentence[index][-1],\n",
    "    'suffix-2': sentence[index][-2:],\n",
    "    'suffix-3': sentence[index][-3:],\n",
    "    'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "    'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "    'has_hyphen': '-' in sentence[index],\n",
    "    'is_numeric': sentence[index].isdigit(),\n",
    "    'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    \"\"\"tagged_sentences:每个单词都有词性标注\n",
    "    X是由features构建的每个词的特征\n",
    "    y是每个词的词性\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('understand', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('share', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('compassion', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('*T*-2', '-NONE-'),\n",
       " ('makes', 'VBZ'),\n",
       " ('judges', 'NNS'),\n",
       " ('sometimes', 'RB'),\n",
       " ('wish', 'VB'),\n",
       " ('*-3', '-NONE-'),\n",
       " ('to', 'TO'),\n",
       " ('offer', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('kind', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Solomonic', 'JJ'),\n",
       " ('aid', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('those', 'DT'),\n",
       " ('who', 'WP'),\n",
       " ('*T*-4', '-NONE-'),\n",
       " (\"'ve\", 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('hurt', 'VBN'),\n",
       " ('*-1', '-NONE-'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('classifier', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = transform_to_dataset(training_sentences)\n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "\n",
    "clf.fit(X[:10000], y[:10000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8951068616422947\n",
      "<zip object at 0x7fe81a49c988>\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    "\n",
    "print( \"Accuracy:\", clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "def pos_tag(sentence):\n",
    "    tagged_sentence = []\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    return zip(sentence, tags)\n",
    "\n",
    "\n",
    "print(pos_tag(word_tokenize('This is my friend, John.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
