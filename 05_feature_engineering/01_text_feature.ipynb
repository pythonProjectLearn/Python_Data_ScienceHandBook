{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造文本特征 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 bag of words 装袋模型，对文本样本来说，有为1， 没有为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 1 1 0]\n",
      " [1 1 0 1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2), min_df=1)\n",
    "# 两个单词相当于样本\n",
    "counts = ngram_vectorizer.fit_transform(['words', 'wprds'])\n",
    "# 给出8个特征，包含特征的为1，不包含的为0\n",
    "ngram_vectorizer.get_feature_names() == ([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'])\n",
    "print counts.toarray().astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo')\n",
      "('is', 'a', 'foo', 'bar')\n",
      "('a', 'foo', 'bar', 'sentences')\n",
      "('foo', 'bar', 'sentences', 'and')\n",
      "('bar', 'sentences', 'and', 'i')\n",
      "('sentences', 'and', 'i', 'want')\n",
      "('and', 'i', 'want', 'to')\n",
      "('i', 'want', 'to', 'ngramize')\n",
      "('want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "n = 4 # you can give 4, 5, 1 or any number less than sentences length\n",
    "ngramsres = ngrams(sentence.split(), n)\n",
    "for grams in ngramsres:\n",
    "  print grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 one-hot编码：对离散变量的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   name age-group\n",
      "0  rick     young\n",
      "1  phil       old\n",
      "\n",
      "----By using Panda ----\n",
      "\n",
      "   name_phil  name_rick  age-group_old  age-group_young\n",
      "0          0          1              0                1\n",
      "1          1          0              1                0\n",
      "\n",
      "----By using Sikit-learn ----\n",
      "\n",
      "{'country=US': 2, 'country=CAN': 0, 'country=MEX': 1}\n",
      "[[ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "df = pd.DataFrame([['rick','young'],['phil','old']],columns=['name','age-group'])\n",
    "print df\n",
    "print \"\\n----By using Panda ----\\n\"\n",
    "print pd.get_dummies(df)\n",
    "\n",
    "X = pd.DataFrame({'income': [100000,110000,90000,30000,14000,50000],\n",
    "                  'country':['US', 'CAN', 'US', 'CAN', 'MEX', 'US'],\n",
    "                  'race':['White', 'Black', 'Latino', 'White', 'White', 'Black']})\n",
    "\n",
    "\n",
    "\n",
    "print \"\\n----By using Sikit-learn ----\\n\"\n",
    "v = DictVectorizer()\n",
    "qualitative_features = ['country']\n",
    "X_qual = v.fit_transform(X[qualitative_features].to_dict('records'))\n",
    "print v.vocabulary_\n",
    "print X_qual.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# def get_tokens():\n",
    "#    with open('/home/jalaj/PycharmProjects/NLPython/NLPython/ch5/TFIDFdemo/shakes/shakes1.txt', 'r') as shakes:\n",
    "#     text = shakes.read()\n",
    "#     lowers = text.lower()\n",
    "#     #remove the punctuation using the character deletion step of translate\n",
    "#     no_punctuation = lowers.translate(None, string.punctuation)\n",
    "#     tokens = nltk.word_tokenize(no_punctuation)\n",
    "#     return tokens\n",
    "\n",
    "# tokens = get_tokens()\n",
    "count = Counter(tokens)  # 分词后的单词\n",
    "#print count.most_common(10)  # 聘数最高的10个单词\n",
    "\n",
    "tokens = get_tokens()\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "count = Counter(filtered)\n",
    "#print count.most_common(100)\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered, stemmer)\n",
    "count = Counter(stemmed)\n",
    "#print count.most_common(100)\n",
    "\n",
    "path = '/home/jalaj/PycharmProjects/NLPython/NLPython/ch5/TFIDFdemo/shakes'\n",
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token_dict[file] = no_punctuation\n",
    "\n",
    "# this can take some time\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "str = 'this sentence has unseen text such as computer but also king lord juliet'\n",
    "response = tfidf.transform([str])\n",
    "#print response\n",
    "\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for col in response.nonzero()[1]:\n",
    "    print feature_names[col], ' - ', response[0, col]\n",
    "\n",
    "\n",
    "feature_array = np.array(tfidf.get_feature_names())\n",
    "tfidf_sorting = np.argsort(response.toarray()).flatten()[::-1]\n",
    "n = 3\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "print top_n\n",
    "\n",
    "n = 4\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "print top_n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
